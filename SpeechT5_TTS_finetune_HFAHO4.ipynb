{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjjacZVF/nruoj28fjcCUd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlesR-W/HFCourses/blob/main/SpeechT5_TTS_finetune_HFAHO4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVGTuimmR9Yp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets soundfile speechbrain accelerate\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset = load_dataset(\"thiagolira/LatinYoutube\")\n",
        "len(dataset)\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "dataset = dataset[\"train\"]"
      ],
      "metadata": {
        "id": "0hBwza5VSKDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor\n",
        "\n",
        "checkpoint = \"microsoft/speecht5_tts\"\n",
        "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
        "tokenizer = processor.tokenizer"
      ],
      "metadata": {
        "id": "GqIArCfUSRES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
      ],
      "metadata": {
        "id": "wGcGalhaTBnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_chars = dataset_vocab - tokenizer_vocab\n",
        "#print(f\"Invalid characters: {invalid_chars}\")\n",
        "for c in invalid_chars:\n",
        "  print(f\"(\\\"{c}\\\",\\\"\\\" ),\")"
      ],
      "metadata": {
        "id": "Ej0ayLlRUC5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacements = [\n",
        "    (\"‚Äò\",\"\" ),\n",
        "    (\"≈å\",\"OO\" ),\n",
        "    (\"\\\\\",\"\" ),\n",
        "    (\"√∫\",\"u\" ),\n",
        "    (\"»≥\",\"yy\" ),\n",
        "    (\"3\",\"tres\" ),\n",
        "    (\"√≥\",\"o\" ),\n",
        "    (\"*\",\"\" ),\n",
        "    (\"8\",\"octo\" ),\n",
        "    (\"ƒ™\",\"II\" ),\n",
        "    (\"7\",\"septem\" ),\n",
        "    (\"≈ç\",\"oo\" ),\n",
        "    (\"0\",\"nullus\" ),\n",
        "    (\"·ºë\",\"he\" ),\n",
        "    (\"Œπ\",\"i\" ),\n",
        "    (\"5\",\"quinque\" ),\n",
        "    (\"‚Äù\",\"\" ),\n",
        "    (\"¬≠\",\"\" ),\n",
        "    (\"‚Äô\",\"\" ),\n",
        "    (\"‚Äì\",\"\" ),\n",
        "    (\"≈´\",\"uu\" ),\n",
        "    (\"4\",\"quattor\" ),\n",
        "    (\"œÉ\",\"s\" ),\n",
        "    (\"\",\"\" ),\n",
        "    (\"≈™\",\"UU\" ),\n",
        "    (\"·Ω∏\",\"\" ),\n",
        "    (\"ƒí\",\"EE\" ),\n",
        "    (\"1\",\"unus\" ),\n",
        "    (\"¬∞\",\"\" ),\n",
        "    (\"·ΩÅ\",\"oo\" ),\n",
        "    (\"ƒ´\",\"ii\" ),\n",
        "    (\"œÅ\",\"r\" ),\n",
        "    (\"√ü\",\"ss\" ),\n",
        "    (\"ƒÄ\",\"AA\" ),\n",
        "    (\" \",\"\" ),\n",
        "    (\"|\",\"\" ),\n",
        "    (\"œÇ\",\"s\" ),\n",
        "    (\"œå\",\"o\" ),\n",
        "    (\"√≠\",\"i\" ),\n",
        "    (\"œÑ\",\"t\" ),\n",
        "    (\"ƒÅ\",\"aa\" ),\n",
        "    (\"9\",\"novem\" ),\n",
        "    (\"‚Äû\",\"\" ),\n",
        "    (\"ƒì\",\"ee\" ),\n",
        "    (\"Œ¨\",\"a\" ),\n",
        "    (\"√°\",\"a\" ),\n",
        "    (\"Œ∫\",\"k\" ),\n",
        "    (\"Œ±\",\"a\" ),\n",
        "    (\"6\",\"sex\" ),\n",
        "    (\"2\",\"duus\" ),\n",
        "    (\"Œø\",\"o\" ),\n",
        "    (\"‚Äú\",\"\" ),\n",
        "    (\"üòÑ\",\"\" ),\n",
        "    (\"√§\",\"a\" ),\n",
        "    (\"ŒΩ\",\"n\" ),\n",
        "    (\"‚Ä¶\",\"\" ),\n",
        "    ]\n",
        "\n",
        "\n",
        "def cleanup_text(inputs):\n",
        "    for src, dst in replacements:\n",
        "        inputs[\"text\"] = inputs[\"text\"].replace(src, dst)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "dataset = dataset.map(cleanup_text)"
      ],
      "metadata": {
        "id": "eUfnVqWZURhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": device},\n",
        "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
        ")\n",
        "\n",
        "\n",
        "def create_speaker_embedding(waveform):\n",
        "    with torch.no_grad():\n",
        "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
        "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
        "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
        "    return speaker_embeddings"
      ],
      "metadata": {
        "id": "ODyki09BWdqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        text=example[\"text\"],\n",
        "        audio_target=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    # strip off the batch dimension\n",
        "    example[\"labels\"] = example[\"labels\"][0]\n",
        "\n",
        "    # use SpeechBrain to obtain x-vector\n",
        "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
        "\n",
        "    return example\n",
        "\n"
      ],
      "metadata": {
        "id": "vsePjwXZftua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_example = prepare_dataset(dataset[0])\n",
        "list(processed_example.keys())\n",
        "processed_example[\"speaker_embeddings\"].shape\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(processed_example[\"labels\"].T)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gTPC44crfyVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
        "def is_not_too_long(input_ids):\n",
        "    input_length = len(input_ids)\n",
        "    return input_length < 200\n",
        "\n",
        "\n",
        "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "YnMgU8JVfzNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "Ftrqs8GHgAUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "x0aFnmPmgBOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollatorWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
        "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
        "\n",
        "        # collate the inputs and targets into a batch\n",
        "        batch = processor.pad(\n",
        "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
        "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
        "        )\n",
        "\n",
        "        # not used during fine-tuning\n",
        "        del batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        # round down target lengths to multiple of reduction factor\n",
        "        if model.config.reduction_factor > 1:\n",
        "            target_lengths = torch.tensor(\n",
        "                [len(feature[\"input_values\"]) for feature in label_features]\n",
        "            )\n",
        "            target_lengths = target_lengths.new(\n",
        "                [\n",
        "                    length - length % model.config.reduction_factor\n",
        "                    for length in target_lengths\n",
        "                ]\n",
        "            )\n",
        "            max_length = max(target_lengths)\n",
        "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
        "\n",
        "        # also add in the speaker embeddings\n",
        "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
        "\n",
        "        return batch\n",
        "data_collator = TTSDataCollatorWithPadding(processor=processor)\n",
        "\n",
        "from transformers import SpeechT5ForTextToSpeech\n",
        "\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n",
        "from functools import partial\n",
        "\n",
        "# disable cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# set language and task for generation and re-enable cache\n",
        "model.generate = partial(model.generate, use_cache=True)\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"speecht5_finetuned_latinvoice\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"],\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor,\n",
        ")"
      ],
      "metadata": {
        "id": "eon7WG4mgEQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "VF2MmE_ZgSzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "YRR3-GDRgUai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
        "    \"CarolusRenniusVitellius/speecht5_finetuned_latinvoice\"\n",
        ")"
      ],
      "metadata": {
        "id": "mHqO4W0AgZu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[\"test\"][0]\n",
        "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n",
        "text = \"Omnis Galliae in partes tres divisa sunt.\"\n",
        "inputs = processor(text=text, return_tensors=\"pt\")\n",
        "from transformers import SpeechT5HifiGan\n",
        "\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio(speech.numpy(), rate=16000)"
      ],
      "metadata": {
        "id": "w21lOv9JBm2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}